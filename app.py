#  Imports
import streamlit as st
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle
import os
from openai import OpenAI

#  Configura莽茫o da p谩gina
st.set_page_config(page_title="Amazon Kendra PDF Search", layout="wide")
st.title(" Consulta Inteligente ao PDF do Amazon Kendra")

#  Carregar 铆ndice FAISS e chunks
index = faiss.read_index("kendra_index.faiss")
with open("kendra_chunks.pkl", "rb") as f:
    chunks = pickle.load(f)

#  Carregar modelo de embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

#  Configurar OpenAI
openai_api_key = os.getenv("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")
client = OpenAI(api_key=openai_api_key) if openai_api_key else None

#  Inicializar hist贸rico
if "historico" not in st.session_state:
    st.session_state.historico = []

#  Entrada de pergunta
query = st.text_input("Digite sua pergunta:")

# Ч Bot玫es
col1, col2 = st.columns([1, 1])
with col1:
    limpar = st.button("Ч Limpar hist贸rico")
with col2:
    baixar = st.button(" Baixar resultados")

# Ъ Limpar hist贸rico
if limpar:
    st.session_state.historico = []
    st.success("Hist贸rico limpo!")

#  Processar pergunta
if query:
    query_embedding = model.encode([query])
    D, I = index.search(np.array(query_embedding), k=3)

    resultados = [chunks[i] for i in I[0]]
    st.session_state.historico.append((query, resultados))

    st.subheader(" Resultados mais relevantes:")
    for i, texto in enumerate(resultados, 1):
        with st.expander(f"Chunk {i}"):
            st.write(texto)

    #  Gerar resposta com OpenAI
    if client:
        contexto = "\n".join(resultados)
        prompt = f"Com base no seguinte conte煤do, responda  pergunta:\n\n{contexto}\n\nPergunta: {query}"

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            st.subheader(" Resposta gerada com OpenAI:")
            st.write(response.choices[0].message.content)
        except Exception as e:
            st.error(f"Erro ao gerar resposta com OpenAI: {e}")
    else:
        st.warning(" OPENAI_API_KEY n茫o est谩 configurada. Configure para ativar gera莽茫o de resposta.")

#  Exibir hist贸rico
if st.session_state.historico:
    st.subheader(" Hist贸rico de perguntas")
    for pergunta, respostas in st.session_state.historico:
        st.markdown(f"**Pergunta:** {pergunta}")
        for r in respostas:
            st.markdown(f"- {r[:200]}...")

#  Baixar resultados
if baixar and st.session_state.historico:
    conteudo = ""
    for pergunta, respostas in st.session_state.historico:
        conteudo += f"Pergunta: {pergunta}\n"
        for r in respostas:
            conteudo += f"- {r}\n"
        conteudo += "\n"

    with open("resultados_kendra.txt", "w", encoding="utf-8") as f:
        f.write(conteudo)

    with open("resultados_kendra.txt", "rb") as f:
        st.download_button(" Clique para baixar", f, file_name="resultados_kendra.txt")